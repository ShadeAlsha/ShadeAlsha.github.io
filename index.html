<!DOCTYPE HTML>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5JHGS5GJZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-Y5JHGS5GJZ');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Shaden A.">
    <title>Shaden Alshammari</title>
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #2c3e50;
            background: #f8f9fa;
            min-height: 100vh;
            font-size: 15px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .main-card {
            background: white;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            border: 1px solid #e9ecef;
            margin-bottom: 24px;
            overflow: hidden;
        }

        .hero-section {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 40px;
            padding: 40px;
            align-items: center;
        }

        .hero-content h1 {
            font-size: 2.2rem;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 20px;
        }

        .hero-content p {
            font-size: 1rem;
            margin-bottom: 16px;
            color: #5a6c7d;
        }

        .hero-links {
            display: flex;
            gap: 16px;
            margin-top: 24px;
        }

        .hero-links a {
            padding: 10px 20px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-weight: 500;
            font-size: 0.9rem;
            transition: background-color 0.2s ease;
        }

        .hero-links a:hover {
            background: #2980b9;
        }

        .profile-image img {
            width: 100%;
            max-width: 240px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .section {
            background: white;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            border: 1px solid #e9ecef;
            margin-bottom: 24px;
            padding: 32px;
        }

        .section h2 {
            font-size: 1.6rem;
            font-weight: 600;
            margin-bottom: 24px;
            color: #2c3e50;
        }

        .research-grid {
            display: grid;
            gap: 20px;
        }

        .research-item {
            display: grid;
            grid-template-columns: 240px 1fr;
            gap: 24px;
            padding: 24px;
            background: #f8f9fa;
            border-radius: 6px;
            border: 1px solid #e9ecef;
            margin-bottom: 16px;
        }

        .research-item.featured {
            background: #fff3cd;
            border-color: #f39c12;
        }

        .research-image {
            overflow: hidden;
            border-radius: 6px;
        }

        .research-image img {
            width: 100%;
            height: 140px;
            object-fit: cover;
        }

        .research-content h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-bottom: 12px;
            color: #2c3e50;
        }

        .research-content h3 a {
            text-decoration: none;
            color: inherit;
        }

        .research-content h3 a:hover {
            color: #3498db;
        }

        .research-content p {
            font-size: 0.9rem;
            line-height: 1.5;
            color: #5a6c7d;
            margin-bottom: 8px;
        }

        .research-links {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin: 12px 0;
        }

        .research-links a {
            padding: 4px 12px;
            background: #e3f2fd;
            color: #1976d2;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.8rem;
            font-weight: 500;
        }

        .research-links a:hover {
            background: #bbdefb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            background: #e74c3c;
            color: white;
            border-radius: 3px;
            font-size: 0.7rem;
            font-weight: 500;
            margin-left: 8px;
            margin-bottom: 8px;
        }

        .teaching-grid {
            display: grid;
            gap: 18px;
        }

        .teaching-item {
            display: grid;
            grid-template-columns: 160px 1fr;
            gap: 20px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 6px;
            border: 1px solid #e9ecef;
            margin-bottom: 16px;
        }

        .teaching-image img {
            width: 100%;
            height: 90px;
            object-fit: cover;
            border-radius: 4px;
        }

        .teaching-content h3 {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 8px;
            color: #2c3e50;
        }

        .teaching-content p {
            font-size: 0.9rem;
            line-height: 1.4;
            margin-bottom: 6px;
            color: #5a6c7d;
        }

        .footer {
            text-align: center;
            padding: 20px;
            color: #7f8c8d;
            font-size: 0.85rem;
        }

        .footer a {
            color: #3498db;
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        .news-scroll-container {
            max-height: 300px;
            overflow-y: auto;
            background: #f8f9fa;
            border-radius: 6px;
            padding: 16px;
            border: 1px solid #e9ecef;
        }
        
        .news-list {
            list-style: none;
            margin: 0;
            padding: 0;
        }
        
        .news-list li {
            padding: 12px 16px;
            margin-bottom: 8px;
            background: white;
            border-radius: 4px;
            border-left: 3px solid #3498db;
            font-size: 0.9rem;
            line-height: 1.4;
            color: #2c3e50;
        }
        
        .news-list li:last-child {
            margin-bottom: 0;
        }
        
        .news-list li strong {
            color: #2980b9;
            font-weight: 600;
        }
        
        /* Custom scrollbar */
        .news-scroll-container::-webkit-scrollbar {
            width: 6px;
        }
        
        .news-scroll-container::-webkit-scrollbar-track {
            background: #e9ecef;
            border-radius: 3px;
        }
        
        .news-scroll-container::-webkit-scrollbar-thumb {
            background: #bdc3c7;
            border-radius: 3px;
        }

        @media (max-width: 768px) {
            .hero-section {
                grid-template-columns: 1fr;
                text-align: center;
                padding: 30px;
            }

            .hero-content h1 {
                font-size: 1.8rem;
            }

            .research-item,
            .teaching-item {
                grid-template-columns: 1fr;
                text-align: center;
            }

            .research-image img {
                height: 120px;
            }

            .hero-links {
                flex-direction: column;
                align-items: center;
            }

            .section {
                padding: 20px;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="main-card">
            <div class="hero-section">
                <div class="hero-content">
                    <h1>Shaden Alshammari</h1>
                    <p>
                        I'm a PhD student in EECS at 
                        <a href="https://csail.mit.edu/" style="color:#3498db; text-decoration: none; font-weight: 500;">MIT CSAIL</a>, co-advised by <a href="https://groups.csail.mit.edu/vision/torralbalab/" style="color:#3498db; text-decoration: none; font-weight: 500;">Antonio Torralba</a> and <a href="https://billf.mit.edu/" style="color:#3498db; text-decoration: none; font-weight: 500;">William T. Freeman</a>. My research focuses on advancing 
                        self-supervised learning and vision-language models.
                    </p>
                    <p>
                        I completed my B.S. in Mathematics and Computer Science at MIT, where I worked with amazing mentors 
                        <a href="https://www.ri.cmu.edu/ri-faculty/deva-ramanan/" style="color:#3498db; text-decoration: none; font-weight: 500;">Deva Ramanan</a> and 
                        <a href="https://scholar.google.com/citations?user=gDiWvFoAAAAJ" style="color:#3498db; text-decoration: none; font-weight: 500;">Shu Kong</a> at 
                        <a href="https://labs.ri.cmu.edu/av-center/" style="color:#3498db; text-decoration: none; font-weight: 500;">CMU's Argo AI Center</a>, 
                        as well as <a href="http://www.cs.cmu.edu/~abhinavg/" style="color:#3498db; text-decoration: none; font-weight: 500;">Abhinav Gupta</a> and 
                        <a href="https://vdean.github.io" style="color:#3498db; text-decoration: none; font-weight: 500;">Victoria Dean</a> at 
                        <a href="https://www.ri.cmu.edu/" style="color:#3498db; text-decoration: none; font-weight: 500;">CMU's Robotics Institute</a>.
                    </p>
                    <p>
                        Beyond research, I'm active in the math olympiad community as a former contestant (IMO Bronze 2017, EGMO and BMO Gold 2016). I also train students, design problems, and I served as a deputy leader and observer at IMO and EGMO.
                    </p>
                    <div class="hero-links">
                        <a href="mailto:shaden@mit.edu">Email</a>
                        <a href="data/Shaden_CV_.pdf">CV</a>
                        <a href="https://scholar.google.com/citations?user=2GYNli8AAAAJ&hl=en">Scholar</a>
                        <a href="https://github.com/ShadeAlsha">GitHub</a>
                    </div>
                </div>
                <div class="profile-image">
                    <img src="images/shaden.jpg" alt="Profile photo">
                </div>
            </div>
        </div>

        <div class="section">
            <h2>News</h2>
            <div class="news-scroll-container">
                <ul class="news-list">
                    <li> Excited to join <a href="https://tedai-vienna.ted.com" style="color:#D30000; text-decoration: none; font-weight: 500;">TEDAI Vienna</a> as a speaker in <strong>September 2025</strong>!</li>
                    <li> Excited to be a speaker at <a href="https://www.mlss-melbourne.com" style="color:#418877; text-decoration: none; font-weight: 500;">Machine Learning Summer School MLSS</a> in Melbourne in <strong>February 2026</strong>!</li>
                    <li><strong>July 2025:</strong> Delighted to spend a week teaching at the KAUST AI Summer School.</li>
                    <li><strong>May/June 2025:</strong> Gave a talk on I-Con at Microsoft MAIDAP and Princeton Visual AI Lab.</li>
                    <li><strong>March 2025:</strong> Received the Schwarzman College of Computing Fellowship (MIT EECS).</li>
                    <li><strong>28 February 2025:</strong> Awarded EDGE Doctoral Fellowship (Stanford University).</li>
                    <li><strong>26 February 2025:</strong> NegBench has been accepted to CVPR 2025!</li>
                    <li><strong>17 February 2025:</strong> Awarded the Gordon Wu Fellowship (Princeton University).</li>
                    <li><strong>22 January 2025:</strong> I-Con has been accepted to ICLR 2025!</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>Research</h2>
            <div class="research-grid">
                <div class="research-item featured">
                    <div class="research-image">
                        <img src="images/I-Con2.png" alt="Unifying Framework">
                    </div>
                    <div class="research-content">
                        <h3>
                            <a href="https://aka.ms/i-con">I-Con: A Unifying Framework for Representation Learning</a>
                        </h3>
                        <p><strong>Shaden A.</strong>, John R. Hershey, Axel Feldmann, William T. Freeman, Mark Hamilton</p>
                        <div class="research-links">
                            <a href="https://aka.ms/i-con">Project Website</a>
                            <a href="https://youtu.be/UvjTbnFzRac">Video Explainer</a>
                            <a href="https://arxiv.org/pdf/2504.16929">Paper</a>
                            <a href="https://github.com/ShadeAlsha/ICon">Code</a>
                        </div>
                        <span class="badge">ICLR 2025</span>
                        <span class="badge" style="background: #27ae60;">
                          <a href="https://news.mit.edu/2025/machine-learning-periodic-table-could-fuel-ai-discovery-0423" style="color: white; text-decoration: none;">
                           MIT News
                          </a>
                        </span>
                        <p>A unified framework that generalizes loss functions in representation learning, exposing connections across methods and achieving state-of-the-art results in unsupervised image classification on ImageNet-1K.</p>
                    </div>
                </div>

                <div class="research-item">
                    <div class="research-image">
                        <img src="images/VisionLanguageModels.png" alt="Vision Language Models">
                    </div>
                    <div class="research-content">
                        <h3>Vision-Language Models Do Not Understand Negation</h3>
                        <p>Kumail Alhamoud, <strong>Shaden A.</strong>, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi</p>
                        <div class="research-links">
                            <a href="http://negbench.github.io">Project Website</a>
                            <a href="https://youtu.be/oOJxwlKUE8M">Video Explainer</a>
                            <a href="https://arxiv.org/pdf/2501.09425">Paper</a>
                            <a href="https://github.com/m1k2zoo/negbench">Code</a>
                        </div>
                        <span class="badge">CVPR 2025</span>
                        <span class="badge" style="background: #27ae60;">
                          <a href="https://news.mit.edu/2025/study-shows-vision-language-models-cant-handle-negation-words-queries-0514" style="color: white; text-decoration: none;">
                            MIT News
                          </a>
                        </span>
                        <p>A benchmark evaluating negation understanding in vision-language models reveals performance limitations, with targeted improvements increasing recall by 10% and accuracy by 40%.</p>
                    </div>
                </div>

                <div class="research-item featured">
                    <div class="research-image">
                        <img src="images/ltr-wb2.png" alt="Long-tailed Recognition">
                    </div>
                    <div class="research-content">
                        <h3>
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alshammari_Long-Tailed_Recognition_via_Weight_Balancing_CVPR_2022_paper.pdf">Long-tailed Recognition via Weight Balancing</a>
                        </h3>
                        <p><strong>Shaden A.</strong>, Yu-Xiong Wang, Deva Ramanan, Shu Kong</p>
                        <span class="badge">CVPR 2022</span>
                        <span class="badge" style="background: #27ae60;">+200 citations!</span>
                        <p>This study explores weight balancing techniques like L2-normalization, weight decay, and MaxNorm to address bias in long-tailed recognition, achieving state-of-the-art results across five benchmarks by balancing classifier weights for rare and common classes.</p>
                    </div>
                </div>

                <div class="research-item">
                    <div class="research-image">
                        <img src="images/hearning-touch.png" alt="Contact Microphones">
                    </div>
                    <div class="research-content">
                        <h3>
                            <a href="https://neurips.cc/virtual/2022/57568">Using Contact Microphones for Robot Manipulation</a>
                        </h3>
                        <p><strong>Shaden A.</strong>, Victoria Dean, Tess Hellebrekers, Pedro Morgado, Abhinav Gupta</p>
                        <span class="badge" style="background: #8e44ad;">NeurIPS Workshop 2022</span>
                        <p>This work combines visual data with contact audio to enhance manipulation in contact-rich tasks, leveraging high-frequency tactile signals from microphones to outperform single-modality approaches.</p>
                    </div>
                </div>

                <div class="research-item">
                    <div class="research-image">
                        <img src="images/continual-ltr.png" alt="Continual Long-Tailed Recognition">
                    </div>
                    <div class="research-content">
                        <h3>
                            <a href="https://drive.google.com/file/d/1RHc1cEMbl6MdgKOZBa-VM4UG_eHEO9y1/view?usp=sharing">Continual Long-Tailed Recognition: Merge Tail Classes Today, Separate them Tomorrow</a>
                        </h3>
                        <p>Yanan Li, <strong>Shaden A.</strong>, Bin Liu, Shu Kong</p>
                        <span class="badge" style="background: #e91e63;">Preprint 2022</span>
                        <p>This work introduces a continual learning approach for long-tailed recognition, using a Mean-Shift module and Supervised Contrastive loss to improve feature learning and expedite finetuning across time periods, achieving state-of-the-art performance.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Teaching</h2>
            <div class="teaching-grid">
                <div class="teaching-item">
                    <div class="teaching-image">
                        <img src="images/math.png" alt="MIT Mathematics">
                    </div>
                    <div class="teaching-content">
                        <h3>Lead Graduate Instructor, Linear Algebra and Optimization (18.C06)</h3>
                        <p><a href="https://math.mit.edu/" style="color:#3498db; text-decoration: none; font-weight: 500;">MIT Department of Mathematics</a> - Sep 2022 - Jan 2025</p>
                        <p>I teach two weekly recitation sessions to help clarify challenging topics for 38 students and develop weekly handouts and problem sets for a larger group of 180 students. I also coordinate a team of five TAs and three Graders. I was honored to be nominated by my students for the Teaching Awards.</p>
                    </div>
                </div>

                <div class="teaching-item">
                    <div class="teaching-image">
                        <img src="images/kaust-academy.png" alt="AI Summer School">
                    </div>
                    <div class="teaching-content">
                        <h3>Instructor</h3>
                        <p>AI Summer School at KAUST (June, 2025)</p>
                    </div>
                </div>

                <div class="teaching-item">
                    <div class="teaching-image">
                        <img src="images/ml_course.png" alt="MIT EECS">
                    </div>
                    <div class="teaching-content">
                        <h3>Teaching Assistant, Introduction to Machine Learning (6.036)</h3>
                        <p><a href="https://eecs.mit.edu/" style="color:#3498db; text-decoration: none; font-weight: 500;">MIT EECS Department</a> - Jan 2024 - May 2024</p>
                        <p>Supported professors in organizing technical materials on ML topics, conducted weekly recitations, lab sessions, and hosted office hours for student learning support.</p>
                    </div>
                </div>

                <div class="teaching-item">
                    <div class="teaching-image">
                        <img src="images/imo.png" alt="Math Olympiad">
                    </div>
                    <div class="teaching-content">
                        <h3>Math Olympiad Trainer</h3>
                        <p>Deputy Leader and Observer @ IMO & EGMO (2019–2023)</p>
                        <p>Trained students in combinatorics, number theory, algebra, and geometry for the International Math Olympiad (IMO), focusing on advanced problem-solving skills. Additionally, contributed by suggesting problems for exams for team selection tests.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="footer">
        <p>Website design credits to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a></p>
    </div>
</body>
</html>
